debug: False
load_trained: False # load trained model?
resume: False # resume training from checkpoint?
# previous checkpoint directory
prev_ckpt: None 
# model to load from previous checkpoint directory
prev_model: None 
suffix: all_100m
cuda_home: /gpfs/share/apps/cuda/11.4
tokenizer_parallel: true

data:
  train_data:  /gpfs/data/oermannlab/users/lavender/NYUTron_hf/data/processed/nyu_notes/pretrain_split_prod/grouped_tokenized_sentences 
  tokenizer_dir: /gpfs/data/oermannlab/users/lavender/NYUTron_hf/data/processed/tokenizers/vocab50000wordpiece
  max_seq_len: 512
  vocab_size: 50000
  type_vocab_size: 2
  mlm_prob: 0.15

model:
  n_attention_heads: 12
  n_hidden_layers: 12
  hidden_size: 768

training:
  fp16: True # turn off fp16 to prevent loss overflow
  n_epochs: 100
  per_device_train_batch_size: 64
  save_steps: 2000
  save_total_limit: 3
  deepspeed: configs/deepspeed_config_multinode.json
  logging_steps: 500
  prediction_loss_only: True