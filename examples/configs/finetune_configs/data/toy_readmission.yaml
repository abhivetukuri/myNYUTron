tokenized_data_path: data/finetune/toy_readmission/tokenized
num_label: 2 # for classification
truncation: True
is_split_into_words: False
max_length: 512
num_train_samples: 10
num_eval_samples: 10
tokenizer:
  path: data/pretrain/tokenizer_small_synthetic_clinical